### [Define](https://github.com/KIRANKUMAR7296/Library/blob/main/AI/AI.md) | [AI](https://github.com/KIRANKUMAR7296/Library/blob/main/AI/Artificial%20Intelligence.md) | [ML](https://github.com/KIRANKUMAR7296/Library/blob/main/Machine%20Learning/Machine%20Learning%20Models.md) | [Real World Applications](https://github.com/KIRANKUMAR7296/Library/blob/main/Machine%20Learning/IBM%20Machine%20Learning.md) | [Elite](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Primer%20Steps.md) | [Feature Engineering](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Feature%20Engineering.md) | [NLP](https://github.com/KIRANKUMAR7296/Library/blob/main/AI/Natural%20Language%20Processing.md) | [CV](https://github.com/KIRANKUMAR7296/Library/blob/main/AI/Computer%20Vision.md)

---

### [Python](https://github.com/KIRANKUMAR7296/Library/blob/main/Python/Python.md) | [Data Types](https://github.com/KIRANKUMAR7296/Library/blob/main/Python/1.%20Data%20Types.md) | [Pandas](https://github.com/KIRANKUMAR7296/Library/blob/main/Python/Pandas.md) | [NumPy](https://github.com/KIRANKUMAR7296/Library/blob/main/Python/NumPy.md) | [OOP](https://github.com/KIRANKUMAR7296/Library/blob/main/Python/Object%20Oriented%20Programming.md) | [Data Cleaning](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Steps/Data%20Cleaning.md)

---

### [Linear Regression](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Supervised%20Learning/Regression/Linear%20Regression.md) | [Metrics](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Supervised%20Learning/Regression/Regression%20Metrics.md) | [Regularization](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Regularization.md) | [Logistic Regression](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Supervised%20Learning/Classification/Logistic%20Regression.md) | [Decision Tree](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Supervised%20Learning/Decision%20Tree.md) | [Ensemble Techniques](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Supervised%20Learning/Ensemble%20Techniques.md)
 
---
 
### [Confusion Matrix](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Confusion%20Matrix.md) | [Bias and Variance](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Bias%20and%20Variance.md)

---

### [Statistics](https://github.com/KIRANKUMAR7296/Library/blob/main/Statistics/Statistics.md) | [Terms](https://github.com/KIRANKUMAR7296/Library/blob/main/Statistics/Important%20Statistical%20Terms.md) | [Distribution](https://github.com/KIRANKUMAR7296/Library/blob/main/Statistics/Distribution.md) | [Standardization](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Normalization%20vs%20Standardization.md) | [Error](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Error.md)

--- 

### [Cross Validation](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Cross%20Validation.md) | [Multiclass vs Multilabel](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Multi%20Class%20and%20Multi%20Label%20Classification.md) | [Dimensionality Reduction](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Unsupervised%20Learning/Dimensionality%20Reduction.md) | [Algorithm Selection](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Steps/Algorithm%20Selection.md)

---

### [Handle Missing Data](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Missing%20Data.md) | [Detect Outliers](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Outliers.md) | [Encode Categorical Feature](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Categorical.md) | [Handle Imbalanced Data](https://github.com/KIRANKUMAR7296/Library/blob/main/Data%20Science/Imbalanced%20Dataset.md)

### Important Disclaimer
- We try to make out **Model** more **Accurate** by **Tuning** and **Tweaking** the **Parameters**.
- But we cannot make a **100%** Accurate Model.
- **Prediction** ( Continuous ) and **Classification** Models, can never be **Error Free**.

> **Y** = f ( **x** ) + **e**

**Y** : Response Variable | Dependent Variable

**x** : Independent Variable

**e** : Irreducible Error

- Even we make a **100%** Accurate Estimate of **f( x )**, Our Model won't be **Error Free**, known as **Irreducible Error**

### Activation Function
- A Function that takes in the **Weighted Sum** of all the Inputs from **Previous Layer** and Generates Output for **Next Layer**.

| DBMS | RDBMS |
| :--- | :---  |
| Store Data in the form of **File** | Store Data in the form of **Tables** |
| **Hierarchical** arrangement of Data | **Rows** and **Columns** ( **Tables** ) |
| Manage **Data** in Computer | Maintain **Relationships** of **Table** in a **Database** |

| Classification | Clustering |
| :--- | :---  |
| Need **Prior** Knowledge of Data | **No Prior** Knowledge of Data |
| Classify New Sample into known **Classes** | Suggest Groups based on **Patterns** in Data |
| **Decision Tree** | **K Means** |
| **Labelled** Samples | **Unlabelled** Samples |

LDA | PCA
:--- | :---
Linear Discriminant Analysis | Principle Component Analysis
Supervised | Unsupervised

K Means | K Nearest Neighbor
:--- | :---
Unsupervised | Supervised
K : Number of **Clusters** | K : Number of **Nearest** Neighbors
Determine the Distances of Each Data Points to the **Centroid** and Assign each Point to Closest Cluster **Centroid** | **Calculate** Distance between **New** Data Point with **Nearest** K Neighbours.

### Autocorrelation
- The **Correlation** of the `Data Point` with a delayed copy of itself. 
- Temperature of the Day **Today** vs Temperature of the Day **Yesterday** or **Tommorrow**.

### Multicollinearity 
- A Phenomenon in which at least two **Independent Variables** are **Linearly Correlated** ( One can be `Predicted` from the other )

### Cross Join | Cartesian Product
- Generate **Paired** Combination of Each **Row** of **First** Table with each **Row** of the **Second** Table

### K Mean
Step 1 :
1. Select the Number of **K** ( `Clusters` ) you want to Identify in your Data
2. Randomly Select **3** Distinct Data Points as **Clusters** 
3. Distance of Data Points is Calculated from each **Cluster** and it belongs to Closest Cluster.
4. Calculate the Mean ( **Centroid** ) of each Cluster

### Euclidean Distance

### Regression
- Linear Regression 
- Polynomial
- Ridge 
- Lasso
- Elastic Net

### Classification
- Logistic Regression 
- Decision Trees 
- Support Vector Machine 
- K Nearest Neighbors 
- Naive Bayes

### Ensembles
- Bagging ( Random Forest )
- Boosting ( ADABOOST | Gradient | XGBoost )

### Clustering
- K Means Clustering
